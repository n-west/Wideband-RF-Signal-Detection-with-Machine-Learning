{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Minimize a quadratic",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5QTBaIxYEKHCQ7ShRnO3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-west/Wideband-RF-Signal-Detection-with-Machine-Learning/blob/main/1_Minimize_a_quadratic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljWRbsef1tXN"
      },
      "source": [
        "This notebook shows an introduction to autograd by using the simple quadratic function\n",
        "\n",
        "\\begin{equation}\n",
        "y = 2x^2 + 3x + 1\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muGYTSGyeNsr"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thJdtTrZ27ul"
      },
      "source": [
        "Torch has an internal library called autograd which provides automatic differentiation. This is a way of writing code which has the ability to compute *analytic* (rather than numeric) derivatives for functions.\n",
        "\n",
        "We are going to use autograd Variable objects to do math and specify if this is something we want derivatives for or not. The following cell will define our function and plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqvJQdcukCv1"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.autograd.Variable(torch.tensor(2), requires_grad=False)\n",
        "b = torch.autograd.Variable(torch.tensor(3), requires_grad=False)\n",
        "c = torch.autograd.Variable(torch.tensor(1), requires_grad=False)\n",
        "print(a) # Should be 2\n",
        "print(b) # Should be 3\n",
        "print(c) # Should be 1\n",
        "\n",
        "def quadratic(x):\n",
        "  # return x**2 + 1\n",
        "  # return 2*x**2 + 1\n",
        "  return 2*x**2 + 3 * x + 1\n",
        "\n",
        "x_range = torch.autograd.Variable(torch.arange(-5, 5, .1), requires_grad=False)\n",
        "y_range = quadratic(x_range)\n",
        "plt.figure()\n",
        "plt.plot(x_range.detach(), y_range.detach())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-YGSEKj3q64"
      },
      "source": [
        "It is also possible to evaluate the function at specific points and use autograd to show the derivative at that point.\n",
        "\n",
        "Let's start by analytically finding the derivative at $x=5$.\n",
        "\n",
        "\\begin{equation}\n",
        "y = 2x^2 + 3x + 1\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\delta y}{\\delta x} = 4x + 3\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\delta y}{\\delta x}(5) = 23\n",
        "\\end{equation}\n",
        "\n",
        "Now we can use autograd to do the same thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWR5-euhkEry"
      },
      "source": [
        "\n",
        "x_5 = torch.autograd.Variable(torch.ones(1) * 5, requires_grad=True)\n",
        "y_5 = a * torch.pow(x_5, 2) + b * x_5 + c\n",
        "\n",
        "y_5.backward()\n",
        "print(x_5.grad.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDQ0-uW5kUx6"
      },
      "source": [
        "In optimization, we want to find the inputs which minimize the function. We can set the derivative to 0 to find the minimum.\n",
        "\n",
        "\\begin{equation}\n",
        "y = 2x^2 + 3x + 1\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\delta y}{\\delta x} = 4x + 3\n",
        "\\end{equation}\n",
        "\n",
        "The minimum of y be solved by setting $\\frac{\\delta y}{\\delta x} = 0$. \n",
        "\n",
        "\\begin{equation}\n",
        "  x = -3/4\n",
        "\\end{equation}\n",
        "\n",
        "We can also use autograd to iteratively find this minimum. Keep adjusting some guess a little bit based on the derivative until we find the min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ40X-ejkYNN"
      },
      "source": [
        "guess = torch.autograd.Variable(torch.ones(1) * .5, requires_grad=True)\n",
        "for _ in range(25):\n",
        "    y = quadratic(guess)\n",
        "    y.backward()\n",
        "    guess = guess.data - .1 * guess.grad.data\n",
        "\n",
        "    guess = torch.autograd.Variable(guess, requires_grad=True)\n",
        "    print(guess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmlltyLx5JMn"
      },
      "source": [
        "Newton's method is an iterative algorithm to find roots (zeros) which starts with a guess of the minimum. The guess is updated with the following rule:\n",
        "\n",
        "\\begin{equation}\n",
        "x_1 = x - \\frac{f(x)}{f'(x)}\n",
        "\\end{equation}\n",
        "\n",
        "Starting with the analytic derivative we can use autograd to do the update rule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baIZnNGGoDW_"
      },
      "source": [
        "\n",
        "x = torch.autograd.Variable(torch.ones(1) * 5, requires_grad=True)\n",
        "first = True\n",
        "y = quadratic(x)\n",
        "\n",
        "def dquad(x):\n",
        "  return 4*x+3\n",
        "for _ in range(21):\n",
        "  y = dquad(x)\n",
        "  y.backward(retain_graph=True, create_graph=True)\n",
        "  deriv = x.grad.data.detach().clone()\n",
        "  y.backward()\n",
        "  print(f\"{x.data} - {deriv} / {x.grad.data}\")\n",
        "  x = torch.autograd.Variable(x.data - y/(x.grad.data), requires_grad=True)\n",
        "\n",
        "print(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWsATDtctOUw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw_BUB4nLBzR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}