{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 - Fit quadratic",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy6t1ElWYPbs3tFRHFZlZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-west/Wideband-RF-Signal-Detection-with-Machine-Learning/blob/main/2_Fit_quadratic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsp-rNRcd4yf"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7kUXYVqh7AP"
      },
      "source": [
        "Create a random process that we can make noisy observations of. It's a quadratic function with (psuedo-)random numbers as the parameters. Create a quadratic model with autograd parameters that we will fit to match the correct parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRYmsVbYeCZj"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "class quadratic_model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(43)\n",
        "        self.a = torch.nn.Parameter(torch.randn(()))\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))\n",
        "        self.c = torch.nn.Parameter(torch.randn(()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.a * torch.pow(x, 2) + self.b * x + self.c\n",
        "\n",
        "class observed_process():\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(42)\n",
        "        self.a = torch.randn(1)\n",
        "        self.b = torch.randn(1)\n",
        "        self.c = torch.randn(1)\n",
        "\n",
        "    def sample(self, x):\n",
        "        return self.a * torch.pow(x, 2) + self.b * x + self.c\n",
        "\n",
        "    def sample_noisy(self, x):\n",
        "        var = 0.001\n",
        "        return (var*torch.randn(1) + self.a) * torch.pow(x, 2) + (var*torch.randn(1) + self.b) * x + (var*torch.randn(1) + self.c)\n",
        "\n",
        "model = quadratic_model()\n",
        "process = observed_process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-ghx-71iYPZ"
      },
      "source": [
        "Check that the actual (the process we observe) parameters are different than our randomly-initialized model. We are going to train our model parameters to fit the noisy process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-pPIq9DeHzM"
      },
      "source": [
        "print(\"Actual:\")\n",
        "print(process.a)\n",
        "print(process.b)\n",
        "print(process.c)\n",
        "\n",
        "print(\"Before:\")\n",
        "print(model.a)\n",
        "print(model.b)\n",
        "print(model.c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKSicrWowTwg"
      },
      "source": [
        "The following cell \n",
        "\n",
        "1. samples from the noisy process\n",
        "2. runs a forward pass of the model\n",
        "3. computes the loss (mean-squared error)\n",
        "4. runs the backward pass\n",
        "5. updates model parameters by using gradients\n",
        "\n",
        "These steps repeat in a loop continuing to update the model. You can plot the errors for every step. Try changing the **learning rate** and **batch size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrH1wWWahrTW"
      },
      "source": [
        "error = []\n",
        "lr = 0.001\n",
        "for _ in range(20000):\n",
        "    x = (torch.randn(1)*1).detach()\n",
        "    y_target = process.sample_noisy(x)\n",
        "    y_pred = model.forward(x)\n",
        "    model.zero_grad()\n",
        "    \n",
        "    squared_error = torch.mean(torch.square(y_target - y_pred))\n",
        "    \n",
        "    squared_error.backward()\n",
        "    error.append(squared_error.detach())\n",
        "    \n",
        "    model.a.data = model.a.data - lr*model.a.grad.data\n",
        "    model.b.data = model.b.data - lr*model.b.grad.data\n",
        "    model.c.data = model.c.data - lr*model.c.grad.data\n",
        "    \n",
        "print(\"After:\")\n",
        "print(model.a)\n",
        "print(model.b)\n",
        "print(model.c)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La5th33YyGbz"
      },
      "source": [
        "There are more sophisticated rules for using gradients to update each parameter. Deep learning toolboxes provide *optimizers* to provide a consistent API to apply these rules for you. AdamW is usually a very good safe choice. This is the same code that uses AdamW to train this quadratic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7aN8LcyyzYl"
      },
      "source": [
        "# Reinitialize the model to random parameters\n",
        "model = quadratic_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgh8AB3Khs0Q"
      },
      "source": [
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "error = []\n",
        "for _ in range(10000):\n",
        "    x = (torch.randn(1)*1).detach()\n",
        "    y_target = process.sample_noisy(x)\n",
        "    y_pred = model.forward(x)\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    squared_error = torch.mean(torch.square(y_target - y_pred))\n",
        "    \n",
        "    squared_error.backward()\n",
        "    error.append(squared_error.detach())\n",
        "    \n",
        "    optim.step()\n",
        "    \n",
        "print(\"After:\")\n",
        "print(model.a)\n",
        "print(model.b)\n",
        "print(model.c)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeFC7hy39WYY"
      },
      "source": [
        "A very common neural network primitive/layer type is the Linear layer. This is sometimes called the following names:\n",
        "* fully-connected layer\n",
        "* dense layer\n",
        "* linear layer\n",
        "\n",
        "It performs a matrix multiply follow by adding a bias. The number of parameters will be the input shape * output shape + output shape (if using bias). The following shows an example of a small linear layer to demonstrate what is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znnl7pW69Xjk"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "# Parameters will now be  [0.4414,  0.4792, -0.1353] and 0.5304\n",
        "layer_demo = torch.nn.Linear(in_features=3, out_features=1)\n",
        "print(list(layer_demo.parameters()))\n",
        "\n",
        "# The result should be (1 * .4414 + 2 * .4792 + 3 * -.1353) + 0.5304\n",
        "print(layer_demo.forward(torch.tensor([[1.,2.,3.]])))\n",
        "print((1 * .4414 + 2 * .4792 + 3 * -.1353) + 0.5304)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq7fbSJm5iuj"
      },
      "source": [
        "In deep learning we use multi-layer deep neural networks as a highly generalized model. In torch, these primitives are in torch.nn. The following 2-layer network can be fit to the quadratic function. Run the fit and observe how well it matches. Try changing the network features, activation function, batch size, learning rate, add layers, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhwxqTQmy4k0"
      },
      "source": [
        "class nn_quad(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.l1 = torch.nn.Linear(in_features=1, out_features=5)\n",
        "    self.l2 = torch.nn.Linear(in_features=5, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.l1(x))\n",
        "    x = self.l2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfVJQ0hn0SWh"
      },
      "source": [
        "model = nn_quad()\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "error = []\n",
        "for _ in range(50000):\n",
        "    x = (torch.randn(64)*5).detach().unsqueeze(-1)\n",
        "    y_target = process.sample_noisy(x)\n",
        "    y_pred = model.forward(x)\n",
        "    optim.zero_grad()\n",
        "    \n",
        "    squared_error = torch.mean(torch.square(y_target - y_pred))\n",
        "    \n",
        "    squared_error.backward()\n",
        "    error.append(squared_error.detach())\n",
        "    \n",
        "    optim.step()\n",
        "    \n",
        "print(\"After:\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(error)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5JQXV9u0kMx"
      },
      "source": [
        "x_range = torch.autograd.Variable(torch.arange(-5, 5, .025).unsqueeze(-1), requires_grad=False)\n",
        "y_pred = model(x_range)\n",
        "y_actual = process.sample(x_range)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(x_range.detach(), y_pred.detach())\n",
        "plt.plot(x_range.detach(), y_actual.detach())\n",
        "plt.legend([\"Inferred\", \"Actual\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUOMIPdK10Aq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}