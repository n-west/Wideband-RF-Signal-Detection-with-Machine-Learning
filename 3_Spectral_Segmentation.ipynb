{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 - Spectral Segmentation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNP+9UAirc3bC3oJF4GkAnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-west/Wideband-RF-Signal-Detection-with-Machine-Learning/blob/main/3_Spectral_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwdL_0b4MHKY"
      },
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7geQnDVkPtja"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Xhgv5jsuLE"
      },
      "source": [
        "The following will download a small simulation dataset from google drive in to your VM and extract it. This will take ~10-15 minutes. You should run this once early during this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM1NIsxjQQU6"
      },
      "source": [
        "def the_download():\n",
        "  !gdown --id 1IYn3-vmqfMThLa_njyZGLi4YNuNnVZrb\n",
        "  !ls /content/grcon_wideband_train_small.zip\n",
        "  !mkdir /data\n",
        "  !unzip /content/grcon_wideband_train_small.zip -d /data/\n",
        "  !ls /data\n",
        "\n",
        "try:\n",
        "  if DATASET_DOWNLOADED:\n",
        "    print(\"dataset is already downloaded. skipping\")\n",
        "  else:\n",
        "    the_download()\n",
        "    DATASET_DOWNLOADED = True\n",
        "except:\n",
        "  the_download()\n",
        "  DATASET_DOWNLOADED=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztZqEy0ntGIC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wNB55SHMw6p"
      },
      "source": [
        "def segnet_labels(anno, nfft, steps, class_map, nClass, single_class=False):\n",
        "    Y = np.zeros((nfft*steps,nClass+1))\n",
        "\n",
        "    for a in anno:\n",
        "        cur_slice = np.zeros((steps,nfft))\n",
        "        x1 = np.clip(a['x1'], 0, nfft - 1)\n",
        "        x2 = np.clip(a['x2'], 0, nfft - 1)\n",
        "        y1 = np.clip(a['y1'], 0, steps - 1)\n",
        "        y2 = np.clip(a['y2'], 0, steps - 1)\n",
        "\n",
        "        w = x2 - x1 + 1\n",
        "        h = y2 - y1 + 1\n",
        "\n",
        "        if w == 1 and h == 1:\n",
        "            cur_slice[y1,x1] = 1\n",
        "        elif w == 1:\n",
        "            cur_slice[y1:y1+h,x1] = 1\n",
        "        elif h == 1:\n",
        "            cur_slice[y1,x1:x1+w] = 1\n",
        "        else:\n",
        "            cur_slice[y1:y1+h,x1:x1+w] = 1\n",
        "\n",
        "        cur_slice = np.ravel(cur_slice)\n",
        "        l = a['class']\n",
        "\n",
        "        Y[:,class_map[l]+1] += cur_slice\n",
        "\n",
        "    # Background\n",
        "    Y = np.clip(Y,0,1)\n",
        "\n",
        "    idx = np.where(np.sum(Y, axis = 1)==0)[0]\n",
        "    Y[idx,0] = 1\n",
        "\n",
        "    if single_class:\n",
        "        nidx = np.where(not np.sum(Y, axis = 1)==0)[0]\n",
        "        Y[nidx,1] = 1\n",
        "        return Y[:,:,0:2]\n",
        "\n",
        "    return Y\n",
        "\n",
        "class spectrogram_generator:\n",
        "    def __init__(self, basedir, nfft, steps, class_map, batch_size = 16,\n",
        "                 augment = False, norm = False, file_list = None, network = 'segnet',\n",
        "                 empty_anno = True, use_pysinc = False, single_class = False):\n",
        "\n",
        "        self.basedir = basedir\n",
        "        self.nfft = nfft\n",
        "        self.steps = steps\n",
        "        self.class_map = copy.deepcopy(class_map)\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.norm = norm\n",
        "        self.network = network\n",
        "        self.empty_anno = empty_anno\n",
        "        self.use_pysinc = use_pysinc\n",
        "        self.aug = None\n",
        "\n",
        "        self.file_list = file_list\n",
        "\n",
        "        if self.file_list is None:\n",
        "            self.file_list = [f for f in os.listdir(basedir) if f.endswith('sigmf-meta')]\n",
        "\n",
        "        # Loading the annotations\n",
        "        colnames = ['description','fc','fcb','fs','f_low','f_high','bw',\n",
        "                    'start_sample','end_sample','samples','sec','file']\n",
        "\n",
        "        self.annotations = pd.DataFrame(columns=colnames,dtype='object')\n",
        "\n",
        "        for k,f in enumerate(self.file_list):\n",
        "            meta = json.loads(open(os.path.join(basedir,f),'r').read())\n",
        "            anno = meta['annotations']\n",
        "\n",
        "            fc = meta['captures'][0]['core:frequency']\n",
        "            fs = meta['global']['core:sample_rate']\n",
        "\n",
        "            print('Loading Annotations from: ', f)\n",
        "\n",
        "            annocount = {}\n",
        "            for a in anno:\n",
        "                comment = a.get('core:description', \"\")\n",
        "                if single_class:\n",
        "                    comment = 'detection'\n",
        "                if(not comment in annocount):\n",
        "                    annocount[comment] = [0, 0]\n",
        "                if comment not in self.class_map:  # Can control the labels of the generated sample\n",
        "                    annocount[comment][1] += 1 # increment the NOT used count\n",
        "                    continue\n",
        "\n",
        "                annocount[comment][0] += 1 # increment the USED count\n",
        "\n",
        "                f_low   = a['core:freq_lower_edge']\n",
        "                f_high  = a['core:freq_upper_edge']\n",
        "                startsamp = a['core:sample_start']\n",
        "                nsamp   = a['core:sample_count']\n",
        "                bw      = np.abs(f_high-f_low)\n",
        "                sec     = nsamp/float(fs)\n",
        "                fcb     = (f_low+f_high)/2.0\n",
        "\n",
        "                # append to the giant dataframe\n",
        "                nv = pd.DataFrame([[comment,fc,fcb,fs,f_low,f_high,bw,startsamp,startsamp+nsamp,nsamp,sec,f]],\n",
        "                                  columns=colnames)\n",
        "                self.annotations = self.annotations.append(nv)\n",
        "\n",
        "            for k,v in annocount.items():\n",
        "                print(\"%s :: Using %d annotation :: NOT Using %d annotations.\"%(k, v[0], v[1]))\n",
        "\n",
        "        self.get_class_map()\n",
        "        self.inv_map = {v: k for k, v in self.class_map.items()}\n",
        "        self.nClass = len(self.class_map)\n",
        "        print('All Annotations Loaded')\n",
        "\n",
        "    def keras_gen_train(self, maxproc=4):\n",
        "        while True:\n",
        "            X, Y = self.generate(augment=True)\n",
        "            yield X, Y\n",
        "\n",
        "    def keras_gen_val(self, maxproc=4):\n",
        "        while True:\n",
        "            X, Y = self.generate(augment=False)\n",
        "            yield X, Y\n",
        "\n",
        "    def get_class_map(self):\n",
        "      return self.class_map\n",
        "        # n = len(self.class_map)\n",
        "        # keys = sorted(self.class_map.keys()) # Making sure class map keys are consistent\n",
        "        # i = 0\n",
        "        # for k in keys:\n",
        "        #     self.class_map[k] = i\n",
        "        #     i += 1\n",
        "\n",
        "    def generate(self, batch_size=None,deterministic_file=None, deterministic_offset=None,returnearly=False, augment=False):\n",
        "        if batch_size is not None:\n",
        "            self.batch_size=batch_size\n",
        "\n",
        "        batch_X = np.empty((self.batch_size, self.steps* self.nfft, 2), dtype=np.float32)\n",
        "        batch_Y = np.empty((self.batch_size, self.nClass+1, self.steps, self.nfft), dtype=np.float32)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        while count < self.batch_size:\n",
        "\n",
        "            if deterministic_file is None:\n",
        "                f = np.random.choice(self.file_list)\n",
        "            else:\n",
        "                f = deterministic_file\n",
        "            data_file = f.replace('meta','data')\n",
        "            Fsamps = np.memmap(os.path.join(self.basedir,data_file),dtype=np.complex64,mode='r')\n",
        "            anno = self.annotations.loc[self.annotations['file']==f]\n",
        "            nsamp = self.nfft*self.steps\n",
        "\n",
        "            if anno.empty:\n",
        "                # special case to allow empty captures (reduces false positives)\n",
        "                file_max = len(Fsamps)\n",
        "                file_min = 0\n",
        "                offset_start = np.random.randint(max(0,file_min), min(len(Fsamps)-nsamp, file_max) )\n",
        "                offset_end = offset_start + nsamp\n",
        "                samps = Fsamps[offset_start:offset_end]\n",
        "                batch_Y[count, :, :] = 0.0\n",
        "                batch_Y[count, :, 0] = 1.0\n",
        "\n",
        "                batch_X[count,:,0] = samps.real\n",
        "                batch_X[count,:,1] = samps.imag\n",
        "                count += 1\n",
        "                continue\n",
        "\n",
        "            file_min = anno['start_sample'].min()\n",
        "            file_max = anno['end_sample'].max()\n",
        "\n",
        "            fs = list(anno['fs'])[0]\n",
        "            fc = list(anno['fc'])[0]\n",
        "            (fc0,fs0) = (fc,fs)\n",
        "\n",
        "            if deterministic_offset is None:\n",
        "                offset_start = np.random.randint(max(0,file_min),min(len(Fsamps)-nsamp, file_max) )\n",
        "            else:\n",
        "                offset_start = deterministic_offset\n",
        "            offset_end = offset_start+nsamp\n",
        "            samps = Fsamps[offset_start:offset_end]/np.float32(65536.0)\n",
        "            if augment:\n",
        "                samps += np.complex64(np.random.uniform(1e-9, 1e-7) * (np.random.randn(offset_end-offset_start) + 1.j*np.random.randn(offset_end-offset_start)))\n",
        "            else:\n",
        "              pass\n",
        "                # samps += np.complex64(1e-5 * (np.random.randn(offset_end-offset_start) + 1.j*np.random.randn(offset_end-offset_start)))\n",
        "\n",
        "            rel_anno = anno.loc[anno['end_sample'] > offset_start]\n",
        "            rel_anno = rel_anno.loc[rel_anno['start_sample'] < offset_end]\n",
        "            rel_anno = rel_anno.loc[rel_anno['f_high'] > fc-fs/2.0]\n",
        "            rel_anno = rel_anno.loc[rel_anno['f_low'] < fc+fs/2.0]\n",
        "\n",
        "            batch_X[count,:,0] = samps.real\n",
        "            batch_X[count,:,1] = samps.imag\n",
        "\n",
        "            records = []\n",
        "            for _,r in rel_anno.iterrows():\n",
        "                rng = max(offset_start,r['start_sample']),min(offset_end,r['end_sample'])\n",
        "                rng = list(map(lambda x: ((x-offset_start)/self.nfft), rng))\n",
        "\n",
        "                band_low = (fc-fs/2.0)\n",
        "                frac_low  = (r['f_low'] - band_low)/fs\n",
        "                frac_high = (r['f_high'] - band_low)/fs\n",
        "\n",
        "                frac_low = max(0,frac_low)\n",
        "                frac_high = min(1, frac_high)\n",
        "\n",
        "                assert(frac_low <= 1)\n",
        "                assert(frac_low >= 0)\n",
        "                assert(frac_high <= 1)\n",
        "                assert(frac_high >= 0)\n",
        "\n",
        "                frng = (frac_low, frac_high)\n",
        "                frng = list(map(lambda x: (x)*self.nfft, frng))\n",
        "                records.append({'x1':int(frng[0]), 'x2':int(frng[1]),\n",
        "                                'y1':int(rng[0]), 'y2':int(rng[1]),\n",
        "                           'class':r['description']})\n",
        "\n",
        "\n",
        "            # Annotations goes to Label Generator\n",
        "            batch_y_data = segnet_labels(records,self.nfft,self.steps,self.class_map, self.nClass)\n",
        "            batch_y_data = np.transpose(batch_y_data, (1,0)).reshape((self.nClass+1, self.steps, self.nfft))\n",
        "            batch_Y[count,:,:,:] = batch_y_data\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        return batch_X, batch_Y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWMJdqhA4qUZ"
      },
      "source": [
        "basedir = '/data/grcon_train/'\n",
        "\n",
        "labels = []\n",
        "file_list = [f for f in os.listdir(basedir) if f.endswith('sigmf-meta')]\n",
        "for fname in file_list:\n",
        "  with open(basedir + fname, mode=\"r\") as md_f:\n",
        "    md = json.loads(md_f.read())\n",
        "    for anno in md[\"annotations\"]:\n",
        "      if anno[\"core:description\"] not in labels:\n",
        "        labels.append(anno[\"core:description\"])\n",
        "print(labels)\n",
        "\n",
        "class_map = {\n",
        "            \"PSK2\": 1,\n",
        "            \"PSK4\": 1,\n",
        "            \"PSK8\": 1,\n",
        "            \"QAM16\": 1,\n",
        "            \"QAM64\": 1,\n",
        "            \"QAM256\": 1,\n",
        "            \"OOK\": 1,\n",
        "            \"FSK2\": 1,\n",
        "            \"FSK4\": 1,\n",
        "            \"GMSK\": 1,\n",
        "            \"OFDM\": 1,\n",
        "            \"AM_SSB\": 1,\n",
        "            \"AM_DSB\": 1,\n",
        "            \"FM\": 1,\n",
        "            }\n",
        "\n",
        "batch_size = 32\n",
        "nfft = 512\n",
        "steps = 128\n",
        "\n",
        "train_gen = spectrogram_generator(basedir, nfft=nfft, steps=steps, \n",
        "                                  class_map=class_map, file_list=file_list,\n",
        "                                  batch_size=batch_size, norm=True)\n",
        "\n",
        "nClass = len(train_gen.class_map)+1 # +1 for the background\n",
        "# nClass = 2\n",
        "\n",
        "training_generator = train_gen.keras_gen_train()\n",
        "validation_generator = train_gen.keras_gen_val()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxYrFzykOf0N"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size=(3, 3)):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.op = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, padding=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(output_channels, output_channels, kernel_size=kernel_size, padding=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class OmninetDown(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size=(3, 3)):\n",
        "        super(OmninetDown, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.conv = DoubleConv(input_channels, output_channels, kernel_size=kernel_size)\n",
        "        self.deepsig_actual_output_size = output_channels + input_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        pool = self.pool(x)\n",
        "        return pool, torch.cat([pool, self.conv(pool)], 1)\n",
        "\n",
        "\n",
        "class OmninetUp(nn.Module):\n",
        "    def __init__(self, input_channels, intermediate_channels, bridge_channels, output_channels, kernel_size=(3, 3)):\n",
        "        super(OmninetUp, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(input_channels, intermediate_channels, kernel_size=(2, 2), stride=2, bias=False)\n",
        "        self.conv = DoubleConv(intermediate_channels+bridge_channels, output_channels, kernel_size=kernel_size)\n",
        "        self.deepsig_actual_output_channels = output_channels + intermediate_channels\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        upsampled = self.upsample(x)\n",
        "        combined = torch.cat([upsampled, bridge], 1)\n",
        "        conved = self.conv(combined)\n",
        "        return torch.cat([conved, upsampled], 1)\n",
        "\n",
        "class UnetInput(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(UnetInput, self).__init__()\n",
        "        self.firstconv = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, output_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(output_channels, output_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(output_channels, output_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.deepsig_actual_output_channels =  2*output_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_conv = self.firstconv(x)\n",
        "        return torch.cat([input_conv, self.residual(input_conv)], 1)\n",
        "\n",
        "\n",
        "class OmninetOutput(nn.Module):\n",
        "    def __init__(self, input_channels, number_classes):\n",
        "        super(OmninetOutput, self).__init__()\n",
        "        self._number_classes = number_classes\n",
        "        self.op = nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels, 16, kernel_size=(2, 2), stride=2, bias=False),\n",
        "                nn.Conv2d(16, 16, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(16, number_classes, kernel_size=(3, 3), padding=(1, 1)),\n",
        "                nn.Softmax(1)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class Omninet(nn.Module):\n",
        "    def __init__(self, input_channels, number_classes):\n",
        "        super(Omninet, self).__init__()\n",
        "        kernel_size = (3, 3)\n",
        "        self.head_layer = UnetInput(input_channels, 16)\n",
        "        self.down_stage1 = OmninetDown(self.head_layer.deepsig_actual_output_channels, 32, kernel_size=kernel_size)\n",
        "        self.down_stage2 = OmninetDown(self.down_stage1.deepsig_actual_output_size, 64, kernel_size=kernel_size)\n",
        "        self.down_stage3 = OmninetDown(self.down_stage2.deepsig_actual_output_size, 64, kernel_size=kernel_size)\n",
        "        self.down_stage4 = OmninetDown(self.down_stage3.deepsig_actual_output_size, 64, kernel_size=kernel_size)\n",
        "\n",
        "        self.upstage_1 = OmninetUp(self.down_stage4.deepsig_actual_output_size, 64, self.down_stage2.deepsig_actual_output_size, 64, kernel_size=(3, 3))\n",
        "        self.upstage_2 = OmninetUp(self.upstage_1.deepsig_actual_output_channels, 64, self.down_stage1.deepsig_actual_output_size, 32, kernel_size=(3, 3))\n",
        "        self.upstage_3 = OmninetUp(self.upstage_2.deepsig_actual_output_channels, 32, self.head_layer.deepsig_actual_output_channels, 16, kernel_size=(3, 3))\n",
        "\n",
        "        self.tail = OmninetOutput(self.upstage_3.deepsig_actual_output_channels, number_classes)\n",
        "\n",
        "        self.nfft = 512\n",
        "        self.steps = 128\n",
        "        self.win = torch.hann_window(self.nfft).reshape((1,1,self.nfft,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.win = self.win.to(x.device)\n",
        "        samples = x.reshape((-1, self.steps, self.nfft, 2)) * self.win\n",
        "        samples = torch.view_as_complex(samples)\n",
        "        psd = torch.fft.fft(samples)\n",
        "        psd = torch.cat((psd[:,:, int(self.nfft/2):], psd[:, :, :int(self.nfft/2)]), -1).unsqueeze(1)\n",
        "\n",
        "        psd = torch.log10(1e-9 + torch.abs(psd[:,:,:]) + torch.abs(psd[:,:,:]))\n",
        "        psd = psd - psd.mean((1,2,3), keepdim=True)\n",
        "        wf = psd / (1e-9 + torch.std(psd, (1,2,3), True, keepdim=True))\n",
        "\n",
        "        head = self.head_layer(wf)\n",
        "        stage1_pool, stage1_out = self.down_stage1(head)\n",
        "        stage2_pool, stage2_out = self.down_stage2(stage1_out)\n",
        "        stage3_pool, stage3_out = self.down_stage3(stage2_out)\n",
        "        stage4_pool, stage4_out = self.down_stage4(stage3_out)\n",
        "\n",
        "        upstage1 = self.upstage_1(stage4_out, stage3_pool)\n",
        "        upstage2 = self.upstage_2(upstage1, stage2_pool)\n",
        "        upstage3 = self.upstage_3(upstage2, stage1_pool)\n",
        "\n",
        "        tail = self.tail(upstage3)\n",
        "        return tail, wf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FICApgK4MSXk"
      },
      "source": [
        "eps = 1e-6\n",
        "device = \"cuda:0\"\n",
        "\n",
        "def wcce(pred, target):\n",
        " return torch.mean(-torch.sum(class_W * target * torch.log(eps+pred), 1))\n",
        "\n",
        "def cce(pred, target):\n",
        " return torch.mean(-torch.sum(target * torch.log(eps+pred), 1))\n",
        "\n",
        "print(nClass)\n",
        "# Avoid blowing up memory in the event a net already exists with refcounts that won't disappear\n",
        "try:\n",
        "  del net\n",
        "except *:\n",
        "  pass\n",
        "\n",
        "net = Omninet(1, nClass)\n",
        "net.to(device)\n",
        "criterion = cce\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-8dIWQjMTiR"
      },
      "source": [
        "checker_generator = train_gen.keras_gen_val()\n",
        "net.to(device)\n",
        "plt.figure(\"loss\")\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13XUrx31MU3S"
      },
      "source": [
        "number_steps = 10\n",
        "net.to(device)\n",
        "for epoch in range(200):\n",
        "    running_train_loss = 0.0\n",
        "    net.train()\n",
        "    for step in range(number_steps):\n",
        "        optimizer.zero_grad()\n",
        "        train_x_array, train_y_array = next(training_generator)\n",
        "        train_x = torch.from_numpy(train_x_array).float().to(device)\n",
        "        \n",
        "        train_y_reshaped = torch.from_numpy(train_y_array).to(device)\n",
        "\n",
        "        train_y_hat, _ = net.forward(train_x)\n",
        "        scoreable_y_hat = train_y_hat.view(-1, nClass, 128, 512)\n",
        "        loss = criterion(scoreable_y_hat, train_y_reshaped)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    train_loss.append(running_train_loss / number_steps)\n",
        "    del loss\n",
        "    del train_y_hat\n",
        "    del scoreable_y_hat\n",
        "    del train_y_reshaped\n",
        "    \n",
        "    running_val_loss = 0.0\n",
        "    net.eval()\n",
        "    for step in range(number_steps):\n",
        "        val_x_array, val_y_array = next(validation_generator)\n",
        "        val_x = torch.from_numpy(val_x_array).float().to(device)\n",
        "        \n",
        "        val_y_reshaped = torch.from_numpy(val_y_array).to(device)\n",
        "\n",
        "        val_y_hat, _ = net(val_x)\n",
        "        scoreable_y_hat = val_y_hat.view(-1, nClass, 128, 512)\n",
        "        \n",
        "        loss = criterion(scoreable_y_hat, val_y_reshaped)\n",
        "        running_val_loss += loss.item()\n",
        "\n",
        "    val_loss.append(running_val_loss / number_steps)\n",
        "    print(running_val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1wjB63ogGS"
      },
      "source": [
        "\n",
        "\n",
        "print(ex_in.shape)\n",
        "print(target.shape)\n",
        "# train_x_array, train_y_array = next(validation_generator)\n",
        "# train_x = torch.from_numpy(train_x_array).float().to(device)\n",
        "\n",
        "# train_y_reshaped = torch.from_numpy(train_y_array).to(device)\n",
        "\n",
        "# train_y_hat, _ = net.forward(train_x)\n",
        "# scoreable_y_hat = train_y_hat.view(-1, nClass, 128, 512)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTSWDgxP7Xqj"
      },
      "source": [
        "scoreable_y_hat.shape\n",
        "print(train_y_array.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCZdaA460B6"
      },
      "source": [
        "ex_in, target = train_gen.generate(augment=False)\n",
        "inferred_y, wf = net.forward(torch.from_numpy(ex_in).to(device))\n",
        "plt.figure(figsize=(8,12))\n",
        "plt.subplot(411)\n",
        "plt.title(\"Inferred bg confidence\")\n",
        "plt.imshow(inferred_y[0,0].detach().cpu().view(128, 512))\n",
        "plt.subplot(412)\n",
        "plt.title(\"Inferred class map\")\n",
        "plt.imshow(inferred_y[0].detach().argmax(0).cpu().view(128, 512), vmin=0, vmax=15, cmap=\"tab20\", interpolation=\"none\")\n",
        "plt.subplot(413)\n",
        "plt.title(\"Target class map\")\n",
        "plt.imshow(target[0].argmax(0), vmin=0, vmax=15, cmap=\"tab20\", interpolation=\"none\")\n",
        "plt.subplot(414)\n",
        "plt.title(\"Waterfall\")\n",
        "plt.imshow(wf[0,0].cpu())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duE3g0vy7hk9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k67I0kyYeBW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}